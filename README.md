# BikeSharing

### Overview

Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able to rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.

The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C

This project is aimed at predicting the bike share amount based on the given data. 

### Exploratory Data Analysis

Visualizing relationships between different variables:  `year`, `month`, `weekday`, `season`, `holiday`, `workingday`, `weather`.

### Work on Dataset

1. 
   Clean the data on training set:

   1. Remove samples where humidity = 0. This is because they all happened in a single day, and the test data didn't have any cases where humidity = 0
   2. Some weather data had values ending with ".5", which made no sense, we removed those samples as well

2. Transform data on training set:

   1. Make a new column `weekday` where the column can take values from `1, 2...7` for `Monday, Tuesday...Sunday`
   2. Remove the `day` column, not useful after step 2.1
   3. Make the target variable `y` on the training set to `log(y+1)`
   4. Make dummies for `year`, `month`, `weekday`, `season`, `holiday`, `workingday`, `weather`. Here we are not dropping the first class for each dummy. (We know for linear regression that if we have for example a dummy for gender = male or female, we need to get only 1 dummy variable where 1=female and 0=male, leaving `k-1` classes for each dummy . **In this case, for our dummies, we are actually leaving `k` classes**)
   5. Scale the variables that are not dummy with `RobustScaler` (a package from `sklearn`), this is similar to normalizing the data, but performs better when there are outliers. More info here: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html
   6. Create interactions for all the variables (2 degree polynomial features). **We went from 39 columns of features to 780 columns**

   

### Model Building / Training

We used XGBoost [(link)](https://xgboost.readthedocs.io/en/stable/), which I far as I know is the state-of-the-art gradient boosting package. (I know Jake used it as well).

1. Hyperparameter tuning using `sklearn`'s `GridSearchCV`: If you just want to replicate our results just use the following hyperparameters: 

   best_params = {'colsample_bytree': 0.3, 'gamma': 0.1, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 500}

2. Fit the model with those parameters using the x features as usual, but remember to use the Y that we transformed to `log(y+1)`

### Predicting

1. All the transformations we did for the training data (just the transformations, not the cleaning), we applied them to the test data. Here be careful: when scaling the test data, you need to use the parameters you used on the training data. Basically you fitted the scaler with the training data. Now you just call .transform() using the test data. 
2. Predict using the model built before. Transform the predictions to `exp(y_predicted)-1` to convert back from `log(y+1)` 